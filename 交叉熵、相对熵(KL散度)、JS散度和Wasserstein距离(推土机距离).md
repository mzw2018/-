## 1. 信息量
任何事件都会承载着一定的信息量，包括已经发生的事件和未发生的事件，只是他们承载的信息量会有所不同。
信息量是一个与事件发生概率相关的概念，事件发生的概率越小，其信息量越大。

定义：
假设 `$X$` 是一个离散型变量，其取值集合为 `$\chi$` ，概率分布函数为：

```math
p(x)=Pr(X=x),  x\in\chi
```


则事件`$X=x_0$` 的信息量为：

```math
I(x_0)=-log(p(x_0))
```
--------------

## 2. 熵
熵就是信息量的期望，熵是对于一个概率分布来说的，而信息量是针对一件特定的事件而言的
假设事件 `$X$` 共有 `$n$` 种可能，发生 `$x_i$`  的概率为 `$p(x_i)$`，则该事件的熵 `$H(X)$` 为：
```math
H(x)=-\sum_{i=1}^{n}p(x_i)log(p(x_i))
```
------------

## 3.  相对熵(KL散度)

对于同一个随机变量 `$x$` 有两个单独的概率分布 `$P(x)$` 和 `$Q(x)$` ，可以使用`KL散度(Kullback-Leibler(KL) divergence)`来衡量这两个分布的差异

在机器学习中，`$P$`往往用来标识样本的真实分布，`$Q$`用来表示模型所预测的分布，那么`KL散度`就可以计算
这两个分布之间的差异，也就是Loss值
```math
D_{KL}(p||q)=\sum_{i=1}^n{p(x_i)log(\frac{p(x_i)}{q(x_i)})}
```

`$Q$`的分布越接近`$P$`，散度值越小，即损失值越小

对数函数是凸函数，所以`KL散度`值为非负数

`KL散度`不是对称的

`KL散度`不满足三角不等式

----------

## 4. 交叉熵
将`KL散度`公式进行变形

```math
D_{KL}(p||q)=\sum_{i=1}^{n}{p(x_i)log(\frac{p(x_i)}{q(x_i)})}=\sum_{i=1}^{n}{p(x_i)log(p(x_i))}-\sum_{i=1}^{n}{p(x_i)log(q(x_i))}=-H(p(x))+[-\sum_{i=1}^{n}{p(x_i)log(q(x_i))}]
```

上式的前一部分是`$p$`的熵，后一部分就是交叉熵：
```math
H(p,q)=-\sum_{i=1}^{n}{p(x_i)log(q(x_i))}
```

在机器学习中，需要评估`label`和`predicts`之间的差距，使用`KL散度`刚刚好，即`$D_{KL}(y||\hat{y})$`。

由于`KL散度`中的前一部分`$-H(y)$`是不变的，故在优化过程中，一般直接使用交叉熵做`loss`评估模型

-------

## 5. JS散度
`JS散度`度量两个概率分布的相似性，是基于`KL散度`的变体，解决了`KL散度`非对称的问题。

一般地，`JS散度`是对称的，其取值在`0`到`1`之间，定义为：
```math
JS(P_1||P_2)=\frac{1}{2}KL(P_1||\frac{P_1+P_2}{2})+\frac{1}{2}KL(P_2||\frac{P_1+P_2}{2})
```

----------------

## 6. Wasserstein距离

`KL散度`和`SL散度`度量的问题：
如果两个分布`$P$`、`$Q$`离得很远，完全没有重叠的部分，那么`KL散度`值是没有意义的，而`JS散度`值是一个常数。这意味着这一点的梯度为`0`

`Wasserstein距离`:度量两个概率分布之间的距离
```math
W(P_1,P_2)=\inf\limits_{\gamma\in\prod(P_1,P_2)}\mathbb{E_{(x,y)\sim\gamma}}[||x-y||]
```

* `$\prod(P_1,P_2)$`是`$P_1$`和`$P_2$`两个分布组合起来的所有可能的联合分布的集合。

* 对于每一个可能的联合分布`$\gamma$`，可以从中采样`$(x,y)\sim\gamma$`得到一个样本x和y，并计算出这对样本的距离`$||x-y||$`，所以可以计算联合分布`$\gamma$`下，样本对距离的期望值`$\mathbb{E_{(x,y)\sim\gamma}}[||x-y||]$`。

* 在所有可能的联合分布中能够对这个期望值取到的下界`$\inf\limits_{\gamma\in\prod(P_1,P_2)}\mathbb{E_{(x,y)\sim\gamma}}[||x-y||]$`就是`Wasserstein距离`

直观上可以把`$\mathbb{E_{(x,y)\sim\gamma}}[||x-y||]$`理解为在`$\gamma$`这个路径规划下把土堆`$P_1$`挪到土堆`$P_2$`所需要的消耗，而`Wasserstein距离`就是在最优路径规划下的最小消耗，所`以Wasserstein距离`又叫`Earth-Mover距离`(`推土机距离`)。

`Wasserstein距离`相比于`KL散度`和`SL散度`的优势在于：

即使两个分布的支撑集没有重叠或者重叠非常小，`Wasserstein距离`仍然能够反映两个分布的远近，而在此情况下`JS散度`是常量，`KL散度`则可能没有意义。

---------------


